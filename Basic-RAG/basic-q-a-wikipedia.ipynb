{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T19:44:47.931076Z",
     "iopub.status.busy": "2024-09-22T19:44:47.930541Z",
     "iopub.status.idle": "2024-09-22T19:44:47.938786Z",
     "shell.execute_reply": "2024-09-22T19:44:47.937422Z",
     "shell.execute_reply.started": "2024-09-22T19:44:47.931018Z"
    }
   },
   "source": [
    "### Kullanılan Donanım\n",
    "\n",
    "- Kaggle Notebook - GPU: 2 x Tesla T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# warnings kütüphanesi uyarıları kapatmak için kullanılır\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPU ve CUDA sürümlerini kontrol etmek için kullanılır\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T11:09:21.068679Z",
     "iopub.status.busy": "2024-11-25T11:09:21.067997Z",
     "iopub.status.idle": "2024-11-25T11:10:03.792255Z",
     "shell.execute_reply": "2024-11-25T11:10:03.791296Z",
     "shell.execute_reply.started": "2024-11-25T11:09:21.068647Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers Installed\n",
      "faiss-cpu Installed\n",
      "torch Installed\n",
      "datasets Installed\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers==4.44.0\n",
    "!echo \"transformers Installed\"\n",
    "!pip install -q faiss-cpu==1.8.0\n",
    "!echo \"faiss-cpu Installed\"\n",
    "!pip install -q torch==2.4.0\n",
    "!echo \"torch Installed\"\n",
    "!pip install -q datasets==2.21.0\n",
    "!echo \"datasets Installed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kütüphane Versiyonları\n",
    "\n",
    "* torch==2.4.0\n",
    "* transformers==4.44.0\n",
    "* datasets==2.21.0\n",
    "* faiss==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_wikipedia_data():\n",
    "    return load_dataset('wikipedia', '20220301.en', split='train[:2]', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_wikipedia_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Len dataset: {len(dataset)}\")\n",
    "print(f\"Sample:\\n{json.dumps(dataset[50630], indent=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_samples = 1000\n",
    "texts = [data['text'] for data in dataset.select(range(min(max_samples, len(dataset))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModel.from_pretrained(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with torch.no_grad()\n",
    "```\n",
    "### Amacı:\n",
    "- torch.no_grad(), PyTorch'ta modelin gradyan hesaplamalarını devre dışı bırakmak için kullanılır. Bu, modelin tahmin (inference) modunda çalıştırıldığı, yani sadece çıktıları hesapladığınız (örneğin embedding, tahmin gibi) durumlarda kullanılır.\n",
    "- Gradyan hesaplamalarını devre dışı bırakmanın nedeni, modelin bu esnada eğitilmediği ve sadece tahmin yapıldığıdır. Gradyan hesaplamaları hafıza açısından pahalı olduğu için, onları kapatarak bellek ve hesaplama süresi tasarrufu sağlarsınız.\n",
    "\n",
    "### Nasıl Çalışır:\n",
    "- Normalde, PyTorch modelleri, her ileri yönlü hesaplamada (forward pass) gradyanları otomatik olarak izler, çünkü bu bilgi geri yayılım (backpropagation) sırasında gereklidir.\n",
    "- Ancak model sadece tahmin için kullanılıyorsa (örneğin, embedding hesaplama veya önceden eğitilmiş bir modeli doğrudan kullanma), gradyan hesaplamasına gerek yoktur. Bu yüzden torch.no_grad() kullanarak gradyan hesaplamalarını devre dışı bırakırız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "```\n",
    "### Amacı:\n",
    "- `outputs.last_hidden_state`, modelin her giriş token'i (kelime) için gizli durumları (hidden states) döndürür.\n",
    "- `.mean(dim=1)`, cümledeki her kelimeye karşılık gelen vektörlerin ortalamasını alarak, cümle için tek bir vektör (embedding) oluşturur.\n",
    "- `.cpu()` işlemi, tensor'u GPU'dan CPU'ya taşır ve `.numpy()` ile numpy formatına dönüştürülür.\n",
    "\n",
    "### Ornek: \n",
    "- Eğer bir cümlede 3 kelime varsa (sequence_length=3) ve her kelime için 4 boyutlu bir vektör (hidden_size=4) varsa, last_hidden_state şöyle bir tensordur:\n",
    "\n",
    "```python\n",
    "last_hidden_state = [\n",
    "   [0.1, 0.2, 0.3, 0.4],  # 1. kelimenin vektörü\n",
    "   [0.5, 0.6, 0.7, 0.8],  # 2. kelimenin vektörü\n",
    "   [0.9, 1.0, 1.1, 1.2]   # 3. kelimenin vektörü\n",
    "]\n",
    "```\n",
    "\n",
    "- .mean(dim=1) ile bu vektörlerin her bir bileşeni için ortalamasını alırız:\n",
    "\n",
    "```python \n",
    "ortalama = [(0.1+0.5+0.9)/3, (0.2+0.6+1.0)/3, (0.3+0.7+1.1)/3, (0.4+0.8+1.2)/3]\n",
    "         = [0.5, 0.6, 0.7, 0.8]\n",
    "    \n",
    "## Output: \n",
    "[0.5, 0.6, 0.7, 0.8]  # Bu cümlenin ortalama vektörü\n",
    "```\n",
    "\n",
    "- Bu kısım, tensor'un GPU’da hesaplanmış olmasından dolayı tensor'u CPU’ya geri taşır. Çünkü bazı işlemler (örneğin, FAISS kullanmak) GPU yerine CPU üzerinde çalışır.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"dimension of embeddings: {embeddings.shape}\")\n",
    "d = embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T11:38:53.485148Z",
     "iopub.status.busy": "2024-11-25T11:38:53.484774Z",
     "iopub.status.idle": "2024-11-25T11:38:56.422178Z",
     "shell.execute_reply": "2024-11-25T11:38:56.421274Z",
     "shell.execute_reply.started": "2024-11-25T11:38:53.485113Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model and tokenizer...\n",
      "Computing embeddings...\n",
      "Embeddings shape: (3, 384)\n",
      "Building FAISS index...\n",
      "Loading generator model and tokenizer...\n",
      "\n",
      "Question: Who designed the Eiffel Tower?\n",
      "Retrieved documents:\n",
      "\n",
      "Document 1:\n",
      "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower....\n",
      "\n",
      "Document 2:\n",
      "The Colosseum is an oval amphitheatre in the centre of the city of Rome, Italy. It is the largest ancient amphitheatre ever built, and is still the largest standing amphitheatre in the world today, despite its age....\n",
      "\n",
      "Document 3:\n",
      "The Great Wall of China is a series of fortifications that were built across the historical northern borders of China to protect and consolidate territories of Chinese states and empires....\n",
      "\n",
      "Document 4:\n",
      "The Colosseum is an oval amphitheatre in the centre of the city of Rome, Italy. It is the largest ancient amphitheatre ever built, and is still the largest standing amphitheatre in the world today, despite its age....\n",
      "\n",
      "Document 5:\n",
      "The Colosseum is an oval amphitheatre in the centre of the city of Rome, Italy. It is the largest ancient amphitheatre ever built, and is still the largest standing amphitheatre in the world today, despite its age....\n",
      "\n",
      "Answer: Gustave Eiffel\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the model names\n",
    "embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "generator_model_name = 'google/flan-t5-base'  # A smaller model suitable for QA\n",
    "\n",
    "# Load the Wikipedia dataset\n",
    "def load_wikipedia_data():\n",
    "    print(\"Loading Wikipedia dataset...\")\n",
    "    return load_dataset('wikipedia', '20220301.en', split='train[:1%]')\n",
    "\n",
    "# dataset = load_wikipedia_data()\n",
    "\n",
    "# # Preprocess the texts\n",
    "# print(\"Preprocessing texts...\")\n",
    "# max_samples = 1000\n",
    "# texts = [data['text'] for data in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "\n",
    "# Load the tokenizer and model for embeddings\n",
    "print(\"Loading embedding model and tokenizer...\")\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name).to(device)\n",
    "\n",
    "# Compute embeddings in batches\n",
    "print(\"Computing embeddings...\")\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    inputs = embedding_tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Concatenate all embeddings\n",
    "embeddings = np.vstack(all_embeddings)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Build the FAISS index\n",
    "print(\"Building FAISS index...\")\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Function to retrieve relevant documents\n",
    "def retrieve(query, k=5):\n",
    "    # Embed the query\n",
    "    query_inputs = embedding_tokenizer(query, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        query_outputs = embedding_model(**query_inputs)\n",
    "    query_embedding = query_outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    # Search in the index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [texts[i] for i in indices[0]]\n",
    "\n",
    "# Load the generator model and tokenizer\n",
    "print(\"Loading generator model and tokenizer...\")\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(generator_model_name)\n",
    "generator_model = AutoModelForSeq2SeqLM.from_pretrained(generator_model_name).to(device)\n",
    "\n",
    "# Function to generate answer\n",
    "def answer_question(question):\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_texts = retrieve(question)\n",
    "    print(\"Retrieved documents:\")\n",
    "    for idx, doc in enumerate(retrieved_texts, 1):\n",
    "        print(f\"\\nDocument {idx}:\\n{doc[:500]}...\")  # Show first 500 characters\n",
    "    # Concatenate retrieved texts\n",
    "    context = ' '.join(retrieved_texts)\n",
    "    # Prepare input for the generator\n",
    "    input_text = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "    # Tokenize input\n",
    "    generator_inputs = generator_tokenizer([input_text], return_tensors='pt', truncation=True, max_length=1024).to(device)\n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        generated_ids = generator_model.generate(\n",
    "            input_ids=generator_inputs['input_ids'],\n",
    "            attention_mask=generator_inputs['attention_mask'],\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    answer = generator_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"Who designed the Eiffel Tower?\"\n",
    "    answer = answer_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T11:22:24.426409Z",
     "iopub.status.busy": "2024-11-25T11:22:24.426059Z",
     "iopub.status.idle": "2024-11-25T11:22:24.950946Z",
     "shell.execute_reply": "2024-11-25T11:22:24.950212Z",
     "shell.execute_reply.started": "2024-11-25T11:22:24.426377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd \n",
    "\n",
    "# Step 1: Load Wikipedia Data (1% sample)\n",
    "# def load_wikipedia_data():\n",
    "#     return load_dataset('wikipedia', '20220301.en', split='train[:2]', trust_remote_code=True)\n",
    "\n",
    "# dataset = load_wikipedia_data()\n",
    "\n",
    "# Step 2: Preprocess and Limit Data Samples\n",
    "# max_samples = 1000\n",
    "# texts = [data['text'] for data in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"document_id\": \"doc1\",\n",
    "        \"text\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\"\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"doc2\",\n",
    "        \"text\": \"The Great Wall of China is a series of fortifications that were built across the historical northern borders of China to protect and consolidate territories of Chinese states and empires.\"\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"doc3\",\n",
    "        \"text\": \"The Colosseum is an oval amphitheatre in the centre of the city of Rome, Italy. It is the largest ancient amphitheatre ever built, and is still the largest standing amphitheatre in the world today, despite its age.\"\n",
    "    }\n",
    "])\n",
    "\n",
    "texts = df['text'].values.tolist()\n",
    "\n",
    "# Step 3: Load the Sentence Transformer Model and Tokenizer\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "# Step 4: Embed the Wikipedia Texts\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Step 5: Build a FAISS Index for Efficient Retrieval\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance index\n",
    "index.add(embeddings)  # Add embeddings to the index\n",
    "\n",
    "# Step 6: Question Answering (without separate retrieve function)\n",
    "query = \"Who designed the Eiffel Tower?\"\n",
    "\n",
    "# Embed the query\n",
    "query_inputs = tokenizer([query], padding=True, truncation=True, return_tensors='pt', max_length=512).to(device)\n",
    "with torch.no_grad():\n",
    "    query_outputs = model(**query_inputs)\n",
    "query_embedding = query_outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Search FAISS index\n",
    "top_k = 5  # Number of passages to retrieve\n",
    "distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "# Retrieve the top-k relevant passages\n",
    "retrieved_passages = [texts[i] for i in indices[0]]\n",
    "\n",
    "# Print the retrieved passages\n",
    "#  \n",
    "for i, passage in enumerate(retrieved_passages):\n",
    "    print(f\"Passage {i + 1}:\\n{passage}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T11:26:33.569082Z",
     "iopub.status.busy": "2024-11-25T11:26:33.568339Z",
     "iopub.status.idle": "2024-11-25T11:26:36.623406Z",
     "shell.execute_reply": "2024-11-25T11:26:36.622495Z",
     "shell.execute_reply.started": "2024-11-25T11:26:33.569047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who designed the Eiffel Tower?\n",
      "Answer: Gustave Eiffel\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model for embeddings\n",
    "embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name).to(device)\n",
    "\n",
    "# Load Wikipedia data\n",
    "def load_wikipedia_data():\n",
    "    return load_dataset('wikipedia', '20220301.en', split='train[:1%]')\n",
    "\n",
    "# dataset = load_wikipedia_data()\n",
    "\n",
    "# # Prepare texts\n",
    "# max_samples = 1000  # You can adjust this number based on your computational resources\n",
    "# texts = [data['text'] for data in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "\n",
    "# Compute embeddings for the documents\n",
    "def compute_document_embeddings(texts):\n",
    "    all_embeddings = []\n",
    "    batch_size = 16  # Adjust based on your GPU memory\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = embedding_tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        all_embeddings.append(embeddings)\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "document_embeddings = compute_document_embeddings(texts)\n",
    "\n",
    "# Build Faiss index\n",
    "d = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(document_embeddings)\n",
    "\n",
    "# Function to get embedding for the query\n",
    "def get_query_embedding(query):\n",
    "    inputs = embedding_tokenizer(query, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "# Retrieve relevant documents\n",
    "def retrieve_documents(query, k=5):\n",
    "    query_embedding = get_query_embedding(query)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [texts[i] for i in indices[0]]\n",
    "\n",
    "# Load QA model\n",
    "qa_model_name = 't5-base'  # You can use a more powerful model if desired\n",
    "qa_tokenizer = T5Tokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(qa_model_name).to(device)\n",
    "\n",
    "# Generate answer using retrieved documents\n",
    "def generate_answer(question, context):\n",
    "    max_input_length = 512  # Maximum input length for T5\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    inputs = qa_tokenizer.encode(input_text, return_tensors='pt', truncation=True, max_length=max_input_length).to(device)\n",
    "    outputs = qa_model.generate(inputs, max_length=150)\n",
    "    answer = qa_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Main function to answer questions\n",
    "def answer_question(question, k=5):\n",
    "    retrieved_texts = retrieve_documents(question, k)\n",
    "    context = ' '.join(retrieved_texts)\n",
    "    answer = generate_answer(question, context)\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question  = \"Who designed the Eiffel Tower?\"\n",
    "    answer = answer_question(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T11:30:32.328225Z",
     "iopub.status.busy": "2024-11-25T11:30:32.327406Z",
     "iopub.status.idle": "2024-11-25T11:30:32.403713Z",
     "shell.execute_reply": "2024-11-25T11:30:32.402779Z",
     "shell.execute_reply.started": "2024-11-25T11:30:32.328191Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting texts into paragraphs...\n",
      "Embedding texts...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "AutoModel is designed to be instantiated using the `AutoModel.from_pretrained(pretrained_model_name_or_path)` or `AutoModel.from_config(config)` methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     27\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(paragraphs, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Step 4: Build an Index with FAISS\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:408\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is designed to be instantiated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing the `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained(pretrained_model_name_or_path)` or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_config(config)` methods.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: AutoModel is designed to be instantiated using the `AutoModel.from_pretrained(pretrained_model_name_or_path)` or `AutoModel.from_config(config)` methods."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel , AutoTokenizer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load and Process Wikipedia Data\n",
    "# print(\"Loading Wikipedia data...\")\n",
    "# max_samples = 1000  # Adjust this number as needed\n",
    "# dataset = load_dataset('wikipedia', '20220301.en', split='train[:1%]')\n",
    "# texts = [data['text'] for data in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "# texts = [text for text in texts if text.strip() != '']\n",
    "\n",
    "# Step 2: Split Texts into Paragraphs\n",
    "print(\"Splitting texts into paragraphs...\")\n",
    "paragraphs = []\n",
    "for text in texts:\n",
    "    paras = text.split('\\n\\n')\n",
    "    paras = [p.strip() for p in paras if p.strip() != '']\n",
    "    paragraphs.extend(paras)\n",
    "\n",
    "# Step 3: Embed the Texts\n",
    "print(\"Embedding texts...\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = AutoModel(model_name, device=device)\n",
    "embeddings = model.encode(paragraphs, batch_size=16, convert_to_numpy=True)\n",
    "\n",
    "# Step 4: Build an Index with FAISS\n",
    "print(\"Building FAISS index...\")\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Step 5: Interactive Q&A Loop\n",
    "print(\"Ready for questions! Type 'exit' to quit.\")\n",
    "\n",
    "# question = input(\"\\nEnter your question: \n",
    "question  = \"Who designed the Eiffel Tower?\"\n",
    "\n",
    "\n",
    "# Retrieve Relevant Paragraphs\n",
    "print(\"Retrieving relevant paragraphs...\")\n",
    "query_embedding = model.encode([question], convert_to_numpy=True)\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "retrieved_paragraphs = [paragraphs[i] for i in indices[0]]\n",
    "\n",
    "# Answer the Question\n",
    "print(\"Answering the question...\")\n",
    "device = 0 if torch.cuda.is_available() else -1  # Set device for QA pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model='distilbert-base-uncased-distilled-squad',\n",
    "    tokenizer='distilbert-base-uncased-distilled-squad',\n",
    "    device=device\n",
    ")\n",
    "answers = []\n",
    "for context in retrieved_paragraphs:\n",
    "    if len(context.strip()) == 0:\n",
    "        continue\n",
    "    result = qa_pipeline({'question': question, 'context': context})\n",
    "    answers.append((result['score'], result['answer']))\n",
    "\n",
    "if not answers:\n",
    "    print(\"No answer found.\")\n",
    "else:\n",
    "    # Sort answers by confidence score\n",
    "    answers.sort(key=lambda x: x[0], reverse=True)\n",
    "    best_answer = answers[0][1]\n",
    "    print(f\"Answer: {best_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T11:27:03.132834Z",
     "iopub.status.busy": "2024-11-25T11:27:03.132476Z",
     "iopub.status.idle": "2024-11-25T11:27:05.916608Z",
     "shell.execute_reply": "2024-11-25T11:27:05.915780Z",
     "shell.execute_reply.started": "2024-11-25T11:27:03.132803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (3, 384)\n",
      "FAISS index contains 3 vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who designed the Eiffel Tower?\n",
      "Answer: Gustave Eiffel\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load and preprocess data\n",
    "def load_wikipedia_data():\n",
    "    return load_dataset('wikipedia', '20220301.en', split='train[:1%]')\n",
    "\n",
    "# dataset = load_wikipedia_data()\n",
    "\n",
    "# max_samples = 1000\n",
    "# texts = [data['text'] for data in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "\n",
    "# 2. Generate embeddings\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def compute_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    dataloader = DataLoader(texts, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "            outputs = model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu())\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    return embeddings.numpy()\n",
    "\n",
    "embeddings = compute_embeddings(texts)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# 3. Build FAISS index\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index contains {index.ntotal} vectors.\")\n",
    "\n",
    "# 4. Retrieval function\n",
    "def retrieve(query, k=5):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(query, return_tensors='pt').to(device)\n",
    "        outputs = model(**inputs)\n",
    "        query_embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = [texts[idx] for idx in indices[0]]\n",
    "    return results\n",
    "\n",
    "# 5. QA model\n",
    "qa_model_name = 't5-base'\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = AutoModelForSeq2SeqLM.from_pretrained(qa_model_name).to(device)\n",
    "\n",
    "def answer_question(question, retrieved_texts):\n",
    "    context = ' '.join(retrieved_texts)\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "    inputs = qa_tokenizer.encode(input_text, return_tensors='pt', truncation=True).to(device)\n",
    "    outputs = qa_model.generate(inputs, max_length=50, num_beams=5, early_stopping=True)\n",
    "    answer = qa_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "def rag_qa(question, k=5):\n",
    "    retrieved_texts = retrieve(question, k)\n",
    "    answer = answer_question(question, retrieved_texts)\n",
    "    return answer\n",
    "\n",
    "# 6. Test the system\n",
    "question = \"Who designed the Eiffel Tower?\"\n",
    "answer = rag_qa(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"document_id\": \"doc1\",\n",
    "        \"text\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\"\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"doc2\",\n",
    "        \"text\": \"The Great Wall of China is a series of fortifications that were built across the historical northern borders of China to protect and consolidate territories of Chinese states and empires.\"\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"doc3\",\n",
    "        \"text\": \"The Colosseum is an oval amphitheatre in the centre of the city of Rome, Italy. It is the largest ancient amphitheatre ever built, and is still the largest standing amphitheatre in the world today, despite its age.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, index_path=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.index = faiss.IndexFlatL2(768)\n",
    "        self.documents = []\n",
    "\n",
    "        if index_path:\n",
    "            self.load_index(index_path)\n",
    "\n",
    "    def load_documents(self, doc_path):\n",
    "        with open(doc_path, 'r') as f:\n",
    "            self.documents = json.load(f)\n",
    "        self.build_index()\n",
    "\n",
    "    def build_index(self):\n",
    "        embeddings = []\n",
    "        for doc in self.documents:\n",
    "            inputs = self.tokenizer(doc['text'], return_tensors='pt', truncation=True, padding=True)\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    def retrieve(self, query, top_k=1):\n",
    "        inputs = self.tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        query_embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        return [self.documents[i] for i in indices[0]]\n",
    "\n",
    "    def save_index(self, index_path):\n",
    "        faiss.write_index(self.index, index_path)\n",
    "\n",
    "    def load_index(self, index_path):\n",
    "        self.index = faiss.read_index(index_path)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "    def generate(self, context, question):\n",
    "        input_text = f\"question: {question} context: {context}\"\n",
    "        inputs = self.tokenizer(input_text, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = self.model.generate(**inputs)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "from retriever import Retriever\n",
    "from generator import Generator\n",
    "\n",
    "def main():\n",
    "    retriever = Retriever()\n",
    "    retriever.load_documents('../data/documents.json')\n",
    "\n",
    "    generator = Generator()\n",
    "\n",
    "    query = \"What is the Eiffel Tower?\"\n",
    "    retrieved_docs = retriever.retrieve(query, top_k=1)\n",
    "    context = retrieved_docs[0]['text']\n",
    "\n",
    "    answer = generator.generate(context, query)\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Data\n",
    "\n",
    "\n",
    "# Retriever\n",
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.index = faiss.IndexFlatL2(768)\n",
    "        self.documents = documents\n",
    "        self.build_index()\n",
    "\n",
    "    def build_index(self):\n",
    "        embeddings = []\n",
    "        for doc in self.documents:\n",
    "            inputs = self.tokenizer(doc['text'], return_tensors='pt', truncation=True, padding=True)\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    def retrieve(self, query, top_k=1):\n",
    "        inputs = self.tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        query_embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        return [self.documents[i] for i in indices[0]]\n",
    "\n",
    "# Generator\n",
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "    def generate(self, context, question):\n",
    "        input_text = f\"question: {question} context: {context}\"\n",
    "        inputs = self.tokenizer(input_text, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = self.model.generate(**inputs)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Main\n",
    "retriever = Retriever()\n",
    "generator = Generator()\n",
    "\n",
    "query = \"What is the Eiffel Tower?\"\n",
    "retrieved_docs = retriever.retrieve(query, top_k=1)\n",
    "context = retrieved_docs[0]['text']\n",
    "\n",
    "answer = generator.generate(context, query)\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "class QAModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "        self.qa_pipeline = pipeline('question-answering', model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def answer_question(self, question, context):\n",
    "        result = self.qa_pipeline(question=question, context=context)\n",
    "        return result['answer']\n",
    "\n",
    "import json\n",
    "from qa_model import QAModel\n",
    "\n",
    "def load_documents(doc_path):\n",
    "    with open(doc_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def main():\n",
    "    documents = \n",
    "    qa_model = QAModel()\n",
    "\n",
    "    query = \"What is the Eiffel Tower?\"\n",
    "    context = next(doc['text'] for doc in documents if \"Eiffel Tower\" in doc['text'])\n",
    "\n",
    "    answer = qa_model.answer_question(query, context)\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
